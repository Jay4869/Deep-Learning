{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MD2NPrQPH24q",
    "outputId": "93068a50-ccce-488d-fd22-56adde7cb112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "78fv3i45H_TS",
    "outputId": "be0a8a24-80bb-4358-f0b6-eac4c06ef9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Deep-Learning\n"
     ]
    }
   ],
   "source": [
    "cd /content/gdrive/My\\ Drive/Deep-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eL1k0joCI9kB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "import time, os, datetime\n",
    "from Model import Residual_Unit\n",
    "from Model import Attention_Block\n",
    "from Model import AttentionResNet56_mini\n",
    "from Model import AttentionResNet56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "XIfzpfL4KAcp",
    "outputId": "87e5b3bf-9748-4c65-a6ed-0bd0b637ef27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n",
      "Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version: \",tf.__version__)\n",
    "print(\"Keras version:\",tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_ntLRVkOsSX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping,TensorBoard, LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "KoXhfftYOvwV",
    "outputId": "e1613e8f-e1f2-4ae6-d24e-253fbfbf3f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30000, 32, 32, 3)\n",
      "y_train shape: (30000, 1)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train[:30000, :, :, :]\n",
    "y_train = y_train[:30000]\n",
    "x_val = x_train[-5000:, :, :, :]\n",
    "y_val = y_train[-5000:]\n",
    "# x_test = x_test[:10000, :, :, :]\n",
    "# y_test = y_test[:10000:]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "Y_val = to_categorical(y_val, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zepH8kprO1w2"
   },
   "outputs": [],
   "source": [
    "# define generators for training and validation data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "# compute quantities required for feature normalization\n",
    "train_datagen.fit(x_train)\n",
    "test_datagen.fit(x_val)\n",
    "test_datagen.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z83oAOUVQxoN",
    "outputId": "8554c62c-3baa-4768-a9e1-e3fb0f24e742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 32, 32, 64)   9472        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 32, 32, 64)   256         conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 32, 32, 64)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 16, 16, 64)   0           activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 16, 16, 64)   256         max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 16, 16, 64)   0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 16, 16, 64)   4160        activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 16, 16, 64)   256         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 16, 16, 64)   0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 16, 16, 64)   36928       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 16, 16, 64)   256         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 16, 16, 64)   0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 16, 16, 256)  16640       activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 16, 16, 256)  16640       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 16, 16, 256)  0           conv2d_233[0][0]                 \n",
      "                                                                 conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 16, 16, 256)  1024        add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 16, 16, 256)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 16, 16, 64)   16448       activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 16, 16, 64)   256         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 16, 16, 64)   0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 16, 16, 64)   36928       activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 16, 16, 64)   256         conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 16, 16, 64)   0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 16, 16, 256)  16640       activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 16, 16, 256)  65792       add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 16, 16, 256)  0           conv2d_237[0][0]                 \n",
      "                                                                 conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 8, 8, 256)    0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 8, 8, 256)    1024        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 8, 8, 256)    0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 8, 8, 64)     16448       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 8, 8, 64)     256         conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 8, 8, 64)     0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 8, 8, 64)     36928       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 8, 8, 64)     256         conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 8, 8, 64)     0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)             (None, 8, 8, 256)    16640       activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, 8, 8, 256)    65792       max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 8, 8, 256)    0           conv2d_249[0][0]                 \n",
      "                                                                 conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 4, 4, 256)    0           add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 4, 4, 256)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)             (None, 4, 4, 64)     16448       activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 4, 4, 64)     256         conv2d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 4, 4, 64)     0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, 4, 4, 64)     36928       activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 4, 4, 64)     256         conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 4, 4, 64)     0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, 4, 4, 256)    16640       activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, 4, 4, 256)    65792       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 4, 4, 256)    0           conv2d_257[0][0]                 \n",
      "                                                                 conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 4, 4, 256)    1024        add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 4, 4, 256)    0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 8, 8, 256)    1024        add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)             (None, 4, 4, 64)     16448       activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 8, 8, 256)    0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 4, 4, 64)     256         conv2d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, 8, 8, 64)     16448       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 4, 4, 64)     0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 8, 8, 64)     256         conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)             (None, 4, 4, 64)     36928       activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 8, 8, 64)     0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 4, 4, 64)     256         conv2d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)             (None, 8, 8, 64)     36928       activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 4, 4, 64)     0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 8, 8, 64)     256         conv2d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)             (None, 4, 4, 256)    16640       activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_262 (Conv2D)             (None, 4, 4, 256)    65792       add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 8, 8, 64)     0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 4, 4, 256)    0           conv2d_261[0][0]                 \n",
      "                                                                 conv2d_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, 8, 8, 256)    16640       activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, 8, 8, 256)    65792       add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 8, 8, 256)    0           add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 8, 8, 256)    0           conv2d_253[0][0]                 \n",
      "                                                                 conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 8, 8, 256)    0           up_sampling2d_8[0][0]            \n",
      "                                                                 add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 8, 8, 256)    1024        add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 8, 8, 256)    0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)             (None, 8, 8, 64)     16448       activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 8, 8, 64)     256         conv2d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 8, 8, 64)     0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)             (None, 8, 8, 64)     36928       activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 16, 16, 256)  1024        add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 8, 8, 64)     256         conv2d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 16, 16, 256)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 8, 8, 64)     0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 16, 16, 64)   16448       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_265 (Conv2D)             (None, 8, 8, 256)    16640       activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_266 (Conv2D)             (None, 8, 8, 256)    65792       add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 16, 16, 64)   256         conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 8, 8, 256)    0           conv2d_265[0][0]                 \n",
      "                                                                 conv2d_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 16, 16, 64)   0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 16, 16, 256)  0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 16, 16, 64)   36928       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_267 (Conv2D)             (None, 16, 16, 256)  65792       up_sampling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 16, 16, 64)   256         conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_268 (Conv2D)             (None, 16, 16, 256)  65792       conv2d_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 16, 16, 64)   0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 16, 16, 256)  0           conv2d_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 16, 16, 256)  16640       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 16, 16, 256)  65792       add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16, 16, 256)  0           activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 16, 16, 256)  0           conv2d_245[0][0]                 \n",
      "                                                                 conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 16, 16, 256)  0           lambda_6[0][0]                   \n",
      "                                                                 add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 16, 16, 256)  1024        multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 16, 16, 256)  0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_269 (Conv2D)             (None, 16, 16, 64)   16448       activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 16, 16, 64)   256         conv2d_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 16, 16, 64)   0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_270 (Conv2D)             (None, 16, 16, 64)   36928       activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 16, 16, 64)   256         conv2d_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 16, 16, 64)   0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_271 (Conv2D)             (None, 16, 16, 256)  16640       activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_272 (Conv2D)             (None, 16, 16, 256)  65792       multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 16, 16, 256)  0           conv2d_271[0][0]                 \n",
      "                                                                 conv2d_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 16, 16, 256)  1024        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 16, 16, 256)  0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_273 (Conv2D)             (None, 16, 16, 128)  32896       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 16, 16, 128)  512         conv2d_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 16, 16, 128)  0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_274 (Conv2D)             (None, 8, 8, 128)    147584      activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 8, 8, 128)    512         conv2d_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 8, 8, 128)    0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_275 (Conv2D)             (None, 8, 8, 512)    66048       activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_276 (Conv2D)             (None, 8, 8, 512)    131584      add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 8, 8, 512)    0           conv2d_275[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 8, 8, 512)    2048        add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 8, 8, 512)    0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_277 (Conv2D)             (None, 8, 8, 128)    65664       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 8, 8, 128)    512         conv2d_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 8, 8, 128)    0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_278 (Conv2D)             (None, 8, 8, 128)    147584      activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 8, 8, 128)    512         conv2d_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 8, 8, 128)    0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)             (None, 8, 8, 512)    66048       activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)             (None, 8, 8, 512)    262656      add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 8, 8, 512)    0           conv2d_279[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 4, 4, 512)    0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 4, 4, 512)    2048        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 4, 4, 512)    0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_289 (Conv2D)             (None, 4, 4, 128)    65664       activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 4, 4, 128)    512         conv2d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 4, 4, 128)    0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_290 (Conv2D)             (None, 4, 4, 128)    147584      activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 4, 4, 128)    512         conv2d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 4, 4, 128)    0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_291 (Conv2D)             (None, 4, 4, 512)    66048       activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_292 (Conv2D)             (None, 4, 4, 512)    262656      max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 4, 4, 512)    0           conv2d_291[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 4, 4, 512)    2048        add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 4, 4, 512)    0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_293 (Conv2D)             (None, 4, 4, 128)    65664       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 4, 4, 128)    512         conv2d_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 4, 4, 128)    0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_294 (Conv2D)             (None, 4, 4, 128)    147584      activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 8, 8, 512)    2048        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 4, 4, 128)    512         conv2d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 8, 8, 512)    0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 4, 4, 128)    0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)             (None, 8, 8, 128)    65664       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 4, 4, 512)    66048       activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 4, 4, 512)    262656      add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 8, 8, 128)    512         conv2d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 4, 4, 512)    0           conv2d_295[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 8, 8, 128)    0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 8, 8, 512)    0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)             (None, 8, 8, 128)    147584      activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 8, 8, 512)    262656      up_sampling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 8, 8, 128)    512         conv2d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 8, 8, 512)    262656      conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 8, 8, 128)    0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 8, 8, 512)    0           conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)             (None, 8, 8, 512)    66048       activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)             (None, 8, 8, 512)    262656      add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 8, 8, 512)    0           activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 8, 8, 512)    0           conv2d_287[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 8, 8, 512)    0           lambda_7[0][0]                   \n",
      "                                                                 add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 8, 8, 512)    2048        multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 8, 8, 512)    0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 8, 8, 128)    65664       activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 8, 8, 128)    512         conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 8, 8, 128)    0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 8, 8, 128)    147584      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 8, 8, 128)    512         conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 8, 8, 128)    0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 8, 8, 512)    66048       activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 8, 8, 512)    262656      multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 8, 8, 512)    0           conv2d_301[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 8, 8, 512)    2048        add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 8, 8, 512)    0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 8, 8, 256)    131328      activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 8, 8, 256)    1024        conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 8, 8, 256)    0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 4, 4, 256)    590080      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 4, 4, 256)    1024        conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 4, 4, 256)    0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 4, 4, 1024)   263168      activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 4, 4, 1024)   525312      add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 4, 4, 1024)   0           conv2d_305[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 4, 4, 1024)   4096        add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 4, 4, 256)    262400      activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 4, 4, 256)    1024        conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 4, 4, 256)    0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 4, 4, 256)    590080      activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 4, 4, 256)    1024        conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 4, 4, 256)    0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 4, 4, 1024)   263168      activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 4, 4, 1024)   1049600     add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 4, 4, 1024)   0           conv2d_309[0][0]                 \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 2, 2, 1024)   0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 2, 2, 1024)   4096        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 2, 2, 1024)   0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 2, 2, 256)    262400      activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 2, 2, 256)    1024        conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 2, 2, 256)    0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 2, 2, 256)    590080      activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 2, 2, 256)    1024        conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 2, 2, 256)    0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 2, 2, 1024)   263168      activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 2, 2, 1024)   1049600     max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 2, 2, 1024)   0           conv2d_321[0][0]                 \n",
      "                                                                 conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 2, 2, 1024)   4096        add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 2, 2, 1024)   0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 2, 2, 256)    262400      activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 2, 2, 256)    1024        conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 2, 2, 256)    0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 2, 2, 256)    590080      activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 4, 4, 1024)   4096        add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 2, 2, 256)    1024        conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 2, 2, 256)    0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 4, 4, 256)    262400      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 2, 2, 1024)   263168      activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 2, 2, 1024)   1049600     add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 4, 4, 256)    1024        conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 2, 2, 1024)   0           conv2d_325[0][0]                 \n",
      "                                                                 conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 4, 4, 256)    0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 4, 4, 1024)   0           add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 4, 4, 256)    590080      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 4, 4, 1024)   1049600     up_sampling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 4, 4, 256)    1024        conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 4, 4, 1024)   1049600     conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 4, 4, 256)    0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 4, 4, 1024)   0           conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 4, 4, 1024)   263168      activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 4, 4, 1024)   1049600     add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 4, 4, 1024)   0           activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 4, 4, 1024)   0           conv2d_317[0][0]                 \n",
      "                                                                 conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 4, 4, 1024)   0           lambda_8[0][0]                   \n",
      "                                                                 add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 4, 4, 1024)   4096        multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 4, 4, 256)    262400      activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 4, 4, 256)    1024        conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 4, 4, 256)    0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 4, 4, 256)    590080      activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 4, 4, 256)    1024        conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 4, 4, 256)    0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 4, 4, 1024)   263168      activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 4, 4, 1024)   1049600     multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 4, 4, 1024)   0           conv2d_331[0][0]                 \n",
      "                                                                 conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 4, 4, 1024)   4096        add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 4, 4, 512)    524800      activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 4, 4, 512)    2048        conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 4, 4, 512)    0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 4, 4, 512)    2359808     activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 4, 4, 512)    2048        conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 4, 4, 512)    0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 4, 4, 2048)   1050624     activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 4, 4, 2048)   2099200     add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 4, 4, 2048)   0           conv2d_335[0][0]                 \n",
      "                                                                 conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 4, 4, 2048)   8192        add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 4, 4, 2048)   4196352     activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 4, 4, 2048)   8192        conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 4, 4, 2048)   37750784    activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 4, 4, 2048)   8192        conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 4, 4, 2048)   4196352     activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 4, 4, 2048)   4196352     add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 4, 4, 2048)   0           conv2d_339[0][0]                 \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 4, 4, 2048)   8192        add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 4, 4, 2048)   4196352     activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 4, 4, 2048)   8192        conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 4, 4, 2048)   37750784    activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 4, 4, 2048)   8192        conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 4, 4, 2048)   4196352     activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 4, 4, 2048)   4196352     add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 4, 4, 2048)   0           conv2d_343[0][0]                 \n",
      "                                                                 conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 1, 1, 2048)   0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           20490       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 126,207,882\n",
      "Trainable params: 126,146,442\n",
      "Non-trainable params: 61,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = AttentionResNet56(shape=(32,32,3), in_channel=64, kernel_size=7, n_classes=10, dropout=0.4, regularization=0.0001)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 130:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 100:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 50:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate:', lr)\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# define loss, metrics, optimizer\n",
    "optimizer = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sS1IW6m_x5Hu",
    "outputId": "ec7874e6-fa21-43b7-bb1b-a2fa554418a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Epoch 1/150\n",
      "234/234 [==============================] - 273s 1s/step - loss: 2.6201 - accuracy: 0.2757 - val_loss: 1.8748 - val_accuracy: 0.2990\n",
      "Learning rate: 0.001\n",
      "Epoch 2/150\n",
      "234/234 [==============================] - 271s 1s/step - loss: 1.8105 - accuracy: 0.3578 - val_loss: 1.6605 - val_accuracy: 0.3992\n",
      "Learning rate: 0.001\n",
      "Epoch 3/150\n",
      "234/234 [==============================] - 270s 1s/step - loss: 1.6700 - accuracy: 0.4012 - val_loss: 1.5625 - val_accuracy: 0.4362\n",
      "Learning rate: 0.001\n",
      "Epoch 4/150\n",
      "234/234 [==============================] - 270s 1s/step - loss: 1.5716 - accuracy: 0.4345 - val_loss: 1.4897 - val_accuracy: 0.4625\n",
      "Learning rate: 0.001\n",
      "Epoch 5/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 1.5112 - accuracy: 0.4572 - val_loss: 1.4350 - val_accuracy: 0.4853\n",
      "Learning rate: 0.001\n",
      "Epoch 6/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.4521 - accuracy: 0.4760 - val_loss: 1.4118 - val_accuracy: 0.4980\n",
      "Learning rate: 0.001\n",
      "Epoch 7/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 1.3976 - accuracy: 0.4984 - val_loss: 1.3142 - val_accuracy: 0.5250\n",
      "Learning rate: 0.001\n",
      "Epoch 8/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.3486 - accuracy: 0.5176 - val_loss: 1.3168 - val_accuracy: 0.5330\n",
      "Learning rate: 0.001\n",
      "Epoch 9/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.3068 - accuracy: 0.5295 - val_loss: 1.2881 - val_accuracy: 0.5458\n",
      "Learning rate: 0.001\n",
      "Epoch 10/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.2812 - accuracy: 0.5413 - val_loss: 1.2468 - val_accuracy: 0.5598\n",
      "Learning rate: 0.001\n",
      "Epoch 11/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.2442 - accuracy: 0.5576 - val_loss: 1.2030 - val_accuracy: 0.5732\n",
      "Learning rate: 0.001\n",
      "Epoch 12/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.2099 - accuracy: 0.5681 - val_loss: 1.1945 - val_accuracy: 0.5797\n",
      "Learning rate: 0.001\n",
      "Epoch 13/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.1871 - accuracy: 0.5770 - val_loss: 1.1524 - val_accuracy: 0.5914\n",
      "Learning rate: 0.001\n",
      "Epoch 14/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 1.1590 - accuracy: 0.5907 - val_loss: 1.1204 - val_accuracy: 0.6041\n",
      "Learning rate: 0.001\n",
      "Epoch 15/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 1.1286 - accuracy: 0.5982 - val_loss: 1.1528 - val_accuracy: 0.5855\n",
      "Learning rate: 0.001\n",
      "Epoch 16/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 1.1029 - accuracy: 0.6090 - val_loss: 1.0797 - val_accuracy: 0.6196\n",
      "Learning rate: 0.001\n",
      "Epoch 17/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 1.0868 - accuracy: 0.6179 - val_loss: 1.0533 - val_accuracy: 0.6274\n",
      "Learning rate: 0.001\n",
      "Epoch 18/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 1.0666 - accuracy: 0.6204 - val_loss: 1.0286 - val_accuracy: 0.6380\n",
      "Learning rate: 0.001\n",
      "Epoch 19/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 1.0354 - accuracy: 0.6322 - val_loss: 1.0530 - val_accuracy: 0.6315\n",
      "Learning rate: 0.001\n",
      "Epoch 20/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 1.0160 - accuracy: 0.6426 - val_loss: 1.0212 - val_accuracy: 0.6432\n",
      "Learning rate: 0.001\n",
      "Epoch 21/150\n",
      "234/234 [==============================] - 272s 1s/step - loss: 0.9979 - accuracy: 0.6464 - val_loss: 0.9950 - val_accuracy: 0.6512\n",
      "Learning rate: 0.001\n",
      "Epoch 22/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.9802 - accuracy: 0.6520 - val_loss: 0.9260 - val_accuracy: 0.6711\n",
      "Learning rate: 0.001\n",
      "Epoch 23/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 0.9668 - accuracy: 0.6570 - val_loss: 0.9110 - val_accuracy: 0.6798\n",
      "Learning rate: 0.001\n",
      "Epoch 24/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.9464 - accuracy: 0.6659 - val_loss: 0.9347 - val_accuracy: 0.6732\n",
      "Learning rate: 0.001\n",
      "Epoch 25/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.9268 - accuracy: 0.6736 - val_loss: 0.9207 - val_accuracy: 0.6734\n",
      "Learning rate: 0.001\n",
      "Epoch 26/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.9147 - accuracy: 0.6812 - val_loss: 0.9041 - val_accuracy: 0.6780\n",
      "Learning rate: 0.001\n",
      "Epoch 27/150\n",
      "234/234 [==============================] - 270s 1s/step - loss: 0.8955 - accuracy: 0.6823 - val_loss: 0.8941 - val_accuracy: 0.6846\n",
      "Learning rate: 0.001\n",
      "Epoch 28/150\n",
      "234/234 [==============================] - 271s 1s/step - loss: 0.8798 - accuracy: 0.6912 - val_loss: 0.8700 - val_accuracy: 0.6949\n",
      "Learning rate: 0.001\n",
      "Epoch 29/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.8663 - accuracy: 0.6975 - val_loss: 0.8356 - val_accuracy: 0.7050\n",
      "Learning rate: 0.001\n",
      "Epoch 30/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.8664 - accuracy: 0.6971 - val_loss: 0.8526 - val_accuracy: 0.7009\n",
      "Learning rate: 0.001\n",
      "Epoch 31/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 0.8383 - accuracy: 0.7071 - val_loss: 0.8121 - val_accuracy: 0.7159\n",
      "Learning rate: 0.001\n",
      "Epoch 32/150\n",
      "234/234 [==============================] - 265s 1s/step - loss: 0.8251 - accuracy: 0.7102 - val_loss: 0.8074 - val_accuracy: 0.7117\n",
      "Learning rate: 0.001\n",
      "Epoch 33/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 0.8228 - accuracy: 0.7115 - val_loss: 0.7947 - val_accuracy: 0.7224\n",
      "Learning rate: 0.001\n",
      "Epoch 34/150\n",
      "234/234 [==============================] - 265s 1s/step - loss: 0.7978 - accuracy: 0.7172 - val_loss: 0.8426 - val_accuracy: 0.7051\n",
      "Learning rate: 0.001\n",
      "Epoch 35/150\n",
      "234/234 [==============================] - 265s 1s/step - loss: 0.7925 - accuracy: 0.7200 - val_loss: 0.7759 - val_accuracy: 0.7236\n",
      "Learning rate: 0.001\n",
      "Epoch 36/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 0.7710 - accuracy: 0.7282 - val_loss: 0.7442 - val_accuracy: 0.7397\n",
      "Learning rate: 0.001\n",
      "Epoch 37/150\n",
      "234/234 [==============================] - 265s 1s/step - loss: 0.7786 - accuracy: 0.7249 - val_loss: 0.7850 - val_accuracy: 0.7226\n",
      "Learning rate: 0.001\n",
      "Epoch 38/150\n",
      "234/234 [==============================] - 265s 1s/step - loss: 0.7588 - accuracy: 0.7333 - val_loss: 0.7546 - val_accuracy: 0.7343\n",
      "Learning rate: 0.001\n",
      "Epoch 39/150\n",
      "234/234 [==============================] - 266s 1s/step - loss: 0.7464 - accuracy: 0.7367 - val_loss: 0.7440 - val_accuracy: 0.7321\n",
      "Learning rate: 0.001\n",
      "Epoch 40/150\n",
      "234/234 [==============================] - 265s 1s/step - loss: 0.7409 - accuracy: 0.7395 - val_loss: 0.7602 - val_accuracy: 0.7369\n",
      "Learning rate: 0.001\n",
      "Epoch 41/150\n",
      "234/234 [==============================] - 271s 1s/step - loss: 0.7360 - accuracy: 0.7417 - val_loss: 0.7359 - val_accuracy: 0.7395\n",
      "Learning rate: 0.001\n",
      "Epoch 42/150\n",
      "234/234 [==============================] - 270s 1s/step - loss: 0.7147 - accuracy: 0.7501 - val_loss: 0.7348 - val_accuracy: 0.7403\n",
      "Learning rate: 0.001\n",
      "Epoch 43/150\n",
      "234/234 [==============================] - 272s 1s/step - loss: 0.7065 - accuracy: 0.7540 - val_loss: 0.7634 - val_accuracy: 0.7341\n",
      "Learning rate: 0.001\n",
      "Epoch 44/150\n",
      "234/234 [==============================] - 270s 1s/step - loss: 0.7069 - accuracy: 0.7506 - val_loss: 0.6696 - val_accuracy: 0.7625\n",
      "Learning rate: 0.001\n",
      "Epoch 45/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.6833 - accuracy: 0.7587 - val_loss: 0.7023 - val_accuracy: 0.7564\n",
      "Learning rate: 0.001\n",
      "Epoch 46/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 0.6754 - accuracy: 0.7634 - val_loss: 0.6804 - val_accuracy: 0.7578\n",
      "Learning rate: 0.001\n",
      "Epoch 47/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 0.6673 - accuracy: 0.7671 - val_loss: 0.6700 - val_accuracy: 0.7654\n",
      "Learning rate: 0.001\n",
      "Epoch 48/150\n",
      "234/234 [==============================] - 272s 1s/step - loss: 0.6725 - accuracy: 0.7638 - val_loss: 0.6532 - val_accuracy: 0.7709\n",
      "Learning rate: 0.001\n",
      "Epoch 49/150\n",
      "234/234 [==============================] - 276s 1s/step - loss: 0.6527 - accuracy: 0.7705 - val_loss: 0.6240 - val_accuracy: 0.7809\n",
      "Learning rate: 0.001\n",
      "Epoch 50/150\n",
      "234/234 [==============================] - 272s 1s/step - loss: 0.6472 - accuracy: 0.7726 - val_loss: 0.6438 - val_accuracy: 0.7723\n",
      "Learning rate: 0.001\n",
      "Epoch 51/150\n",
      "234/234 [==============================] - 270s 1s/step - loss: 0.6395 - accuracy: 0.7742 - val_loss: 0.6706 - val_accuracy: 0.7638\n",
      "Learning rate: 0.0001\n",
      "Epoch 52/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.5927 - accuracy: 0.7923 - val_loss: 0.5417 - val_accuracy: 0.8128\n",
      "Learning rate: 0.0001\n",
      "Epoch 53/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 0.5619 - accuracy: 0.8044 - val_loss: 0.5317 - val_accuracy: 0.8121\n",
      "Learning rate: 0.0001\n",
      "Epoch 54/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5679 - accuracy: 0.8009 - val_loss: 0.5258 - val_accuracy: 0.8145\n",
      "Learning rate: 0.0001\n",
      "Epoch 55/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.5568 - accuracy: 0.8022 - val_loss: 0.5218 - val_accuracy: 0.8177\n",
      "Learning rate: 0.0001\n",
      "Epoch 56/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.5526 - accuracy: 0.8064 - val_loss: 0.5256 - val_accuracy: 0.8129\n",
      "Learning rate: 0.0001\n",
      "Epoch 57/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5471 - accuracy: 0.8083 - val_loss: 0.5201 - val_accuracy: 0.8181\n",
      "Learning rate: 0.0001\n",
      "Epoch 58/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5383 - accuracy: 0.8141 - val_loss: 0.5065 - val_accuracy: 0.8217\n",
      "Learning rate: 0.0001\n",
      "Epoch 59/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5500 - accuracy: 0.8103 - val_loss: 0.5127 - val_accuracy: 0.8212\n",
      "Learning rate: 0.0001\n",
      "Epoch 60/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5385 - accuracy: 0.8119 - val_loss: 0.5119 - val_accuracy: 0.8206\n",
      "Learning rate: 0.0001\n",
      "Epoch 61/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5404 - accuracy: 0.8067 - val_loss: 0.5071 - val_accuracy: 0.8227\n",
      "Learning rate: 0.0001\n",
      "Epoch 62/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5345 - accuracy: 0.8124 - val_loss: 0.5038 - val_accuracy: 0.8236\n",
      "Learning rate: 0.0001\n",
      "Epoch 63/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5345 - accuracy: 0.8138 - val_loss: 0.5086 - val_accuracy: 0.8235\n",
      "Learning rate: 0.0001\n",
      "Epoch 64/150\n",
      "234/234 [==============================] - 267s 1s/step - loss: 0.5283 - accuracy: 0.8165 - val_loss: 0.5008 - val_accuracy: 0.8265\n",
      "Learning rate: 0.0001\n",
      "Epoch 65/150\n",
      "234/234 [==============================] - 269s 1s/step - loss: 0.5340 - accuracy: 0.8134 - val_loss: 0.4986 - val_accuracy: 0.8253\n",
      "Learning rate: 0.0001\n",
      "Epoch 66/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 0.5280 - accuracy: 0.8152 - val_loss: 0.4918 - val_accuracy: 0.8266\n",
      "Learning rate: 0.0001\n",
      "Epoch 67/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 0.5237 - accuracy: 0.8170 - val_loss: 0.4942 - val_accuracy: 0.8267\n",
      "Learning rate: 0.0001\n",
      "Epoch 68/150\n",
      "234/234 [==============================] - 268s 1s/step - loss: 0.5155 - accuracy: 0.8204 - val_loss: 0.4932 - val_accuracy: 0.8271\n",
      "Learning rate: 0.0001\n",
      "Epoch 69/150\n",
      "234/234 [==============================] - 272s 1s/step - loss: 0.5275 - accuracy: 0.8133 - val_loss: 0.4960 - val_accuracy: 0.8270\n",
      "Learning rate: 0.0001\n",
      "Epoch 70/150\n",
      "234/234 [==============================] - 281s 1s/step - loss: 0.5240 - accuracy: 0.8162 - val_loss: 0.4912 - val_accuracy: 0.8298\n",
      "Learning rate: 0.0001\n",
      "Epoch 71/150\n",
      "234/234 [==============================] - 283s 1s/step - loss: 0.5183 - accuracy: 0.8196 - val_loss: 0.4864 - val_accuracy: 0.8312\n",
      "Learning rate: 0.0001\n",
      "Epoch 72/150\n",
      "234/234 [==============================] - 284s 1s/step - loss: 0.5111 - accuracy: 0.8214 - val_loss: 0.4836 - val_accuracy: 0.8326\n",
      "Learning rate: 0.0001\n",
      "Epoch 73/150\n",
      "234/234 [==============================] - 283s 1s/step - loss: 0.5132 - accuracy: 0.8208 - val_loss: 0.4912 - val_accuracy: 0.8288\n",
      "Learning rate: 0.0001\n",
      "Epoch 74/150\n",
      "234/234 [==============================] - 279s 1s/step - loss: 0.5193 - accuracy: 0.8212 - val_loss: 0.4851 - val_accuracy: 0.8293\n",
      "Learning rate: 0.0001\n",
      "Epoch 75/150\n",
      "234/234 [==============================] - 283s 1s/step - loss: 0.5130 - accuracy: 0.8222 - val_loss: 0.4870 - val_accuracy: 0.8295\n",
      "Learning rate: 0.0001\n",
      "Epoch 76/150\n",
      "234/234 [==============================] - 284s 1s/step - loss: 0.5061 - accuracy: 0.8238 - val_loss: 0.4797 - val_accuracy: 0.8317\n",
      "Learning rate: 0.0001\n",
      "Epoch 77/150\n",
      "234/234 [==============================] - 285s 1s/step - loss: 0.5088 - accuracy: 0.8229 - val_loss: 0.4843 - val_accuracy: 0.8315\n",
      "Learning rate: 0.0001\n",
      "Epoch 78/150\n",
      "234/234 [==============================] - 285s 1s/step - loss: 0.5088 - accuracy: 0.8225 - val_loss: 0.4758 - val_accuracy: 0.8338\n",
      "Learning rate: 0.0001\n",
      "Epoch 79/150\n",
      "234/234 [==============================] - 285s 1s/step - loss: 0.5135 - accuracy: 0.8205 - val_loss: 0.4701 - val_accuracy: 0.8359\n",
      "Learning rate: 0.0001\n",
      "Epoch 80/150\n",
      "234/234 [==============================] - 286s 1s/step - loss: 0.5067 - accuracy: 0.8227 - val_loss: 0.4774 - val_accuracy: 0.8336\n",
      "Learning rate: 0.0001\n",
      "Epoch 81/150\n",
      "233/234 [============================>.] - ETA: 0s - loss: 0.5030 - accuracy: 0.8243"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epc = 150\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# training\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "step_size_train = train_generator.n // train_generator.batch_size\n",
    "\n",
    "# validation\n",
    "val_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "step_size_val = val_generator.n // val_generator.batch_size\n",
    "\n",
    "# test\n",
    "test_generator = test_datagen.flow(x_test, y_test, batch_size=batch_size)\n",
    "step_size_test = test_generator.n // test_generator.batch_size\n",
    "\n",
    "# usefull callbacks\n",
    "log_dir=\"Logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=7, min_lr=10e-7, verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_accuracy', patience=15, verbose=1)\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = step_size_train,\n",
    "                    epochs = epc,\n",
    "                    validation_data = val_generator,\n",
    "                    validation_steps = step_size_val,\n",
    "                    callbacks=[tensorboard_callback, lr_reducer, lr_scheduler, early_stopper])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken by above cell is {}.\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gudlovkvOxvd",
    "outputId": "66bab1ab-5a1f-497e-c715-5e3196c1e326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 78s 994ms/step - loss: 2.6543 - accuracy: 0.2826 - val_loss: 2.6961 - val_accuracy: 0.1795\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 78s 997ms/step - loss: 2.5804 - accuracy: 0.2929 - val_loss: 2.6073 - val_accuracy: 0.1835\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 80s 1s/step - loss: 2.3796 - accuracy: 0.3096 - val_loss: 2.1113 - val_accuracy: 0.3071\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 81s 1s/step - loss: 2.3886 - accuracy: 0.3207 - val_loss: 2.5191 - val_accuracy: 0.2915\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 78s 1s/step - loss: 2.4780 - accuracy: 0.3217 - val_loss: 2.5335 - val_accuracy: 0.3053\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 78s 1s/step - loss: 2.2255 - accuracy: 0.3495 - val_loss: 1.9327 - val_accuracy: 0.3928\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 78s 1s/step - loss: 2.1138 - accuracy: 0.3600 - val_loss: 2.1967 - val_accuracy: 0.3854\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 77s 985ms/step - loss: 2.1554 - accuracy: 0.3639 - val_loss: 2.0267 - val_accuracy: 0.3379\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 77s 985ms/step - loss: 2.1729 - accuracy: 0.3639 - val_loss: 2.4712 - val_accuracy: 0.3650\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 77s 981ms/step - loss: 2.1651 - accuracy: 0.3679 - val_loss: 1.8463 - val_accuracy: 0.4117\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 77s 986ms/step - loss: 2.0517 - accuracy: 0.3882 - val_loss: 1.8991 - val_accuracy: 0.3872\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 76s 970ms/step - loss: 2.0346 - accuracy: 0.3859 - val_loss: 1.9276 - val_accuracy: 0.4115\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 76s 975ms/step - loss: 2.0211 - accuracy: 0.4058 - val_loss: 1.8076 - val_accuracy: 0.4259\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 76s 971ms/step - loss: 1.9366 - accuracy: 0.4275 - val_loss: 1.8252 - val_accuracy: 0.4371\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 76s 970ms/step - loss: 1.9116 - accuracy: 0.4224 - val_loss: 2.0880 - val_accuracy: 0.3934\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 76s 977ms/step - loss: 1.8477 - accuracy: 0.4345 - val_loss: 1.7549 - val_accuracy: 0.4555\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 76s 970ms/step - loss: 1.9347 - accuracy: 0.4287 - val_loss: 1.9104 - val_accuracy: 0.4213\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 76s 978ms/step - loss: 1.8976 - accuracy: 0.4346 - val_loss: 1.8026 - val_accuracy: 0.4149\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 76s 974ms/step - loss: 1.8881 - accuracy: 0.4329 - val_loss: 1.9944 - val_accuracy: 0.4553\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 76s 975ms/step - loss: 1.9026 - accuracy: 0.4342 - val_loss: 1.6892 - val_accuracy: 0.4746\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 76s 972ms/step - loss: 1.8924 - accuracy: 0.4396 - val_loss: 2.0318 - val_accuracy: 0.4389\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 77s 981ms/step - loss: 1.7782 - accuracy: 0.4627 - val_loss: 1.6955 - val_accuracy: 0.4826\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 77s 983ms/step - loss: 1.8294 - accuracy: 0.4480 - val_loss: 1.8964 - val_accuracy: 0.4379\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 78s 998ms/step - loss: 2.3653 - accuracy: 0.3772 - val_loss: 1.8380 - val_accuracy: 0.4213\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 77s 984ms/step - loss: 2.7950 - accuracy: 0.3175 - val_loss: 2.2261 - val_accuracy: 0.3746\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 78s 1000ms/step - loss: 2.0450 - accuracy: 0.3947 - val_loss: 1.8746 - val_accuracy: 0.4467\n",
      "Epoch 27/50\n",
      "77/78 [============================>.] - ETA: 0s - loss: 1.8016 - accuracy: 0.4568\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "78/78 [==============================] - 76s 968ms/step - loss: 1.8058 - accuracy: 0.4561 - val_loss: 1.8669 - val_accuracy: 0.4287\n",
      "Epoch 28/50\n",
      "78/78 [==============================] - 76s 977ms/step - loss: 1.6852 - accuracy: 0.4895 - val_loss: 1.5836 - val_accuracy: 0.5110\n",
      "Epoch 29/50\n",
      "78/78 [==============================] - 77s 982ms/step - loss: 1.6794 - accuracy: 0.5004 - val_loss: 1.5432 - val_accuracy: 0.5190\n",
      "Epoch 30/50\n",
      "78/78 [==============================] - 76s 975ms/step - loss: 1.6097 - accuracy: 0.5131 - val_loss: 1.5978 - val_accuracy: 0.5170\n",
      "Epoch 31/50\n",
      "78/78 [==============================] - 76s 977ms/step - loss: 1.6793 - accuracy: 0.5091 - val_loss: 1.4614 - val_accuracy: 0.5415\n",
      "Epoch 32/50\n",
      "78/78 [==============================] - 76s 977ms/step - loss: 1.6167 - accuracy: 0.5190 - val_loss: 1.4812 - val_accuracy: 0.5489\n",
      "Epoch 33/50\n",
      "78/78 [==============================] - 76s 970ms/step - loss: 1.5661 - accuracy: 0.5282 - val_loss: 1.4069 - val_accuracy: 0.5465\n",
      "Epoch 34/50\n",
      "78/78 [==============================] - 75s 965ms/step - loss: 1.6032 - accuracy: 0.5322 - val_loss: 1.4513 - val_accuracy: 0.5427\n",
      "Epoch 35/50\n",
      "78/78 [==============================] - 75s 960ms/step - loss: 1.5891 - accuracy: 0.5389 - val_loss: 1.4244 - val_accuracy: 0.5631\n",
      "Epoch 36/50\n",
      "78/78 [==============================] - 76s 969ms/step - loss: 1.6003 - accuracy: 0.5286 - val_loss: 1.5347 - val_accuracy: 0.5513\n",
      "Epoch 37/50\n",
      "78/78 [==============================] - 75s 961ms/step - loss: 1.5413 - accuracy: 0.5312 - val_loss: 1.4674 - val_accuracy: 0.5505\n",
      "Epoch 38/50\n",
      "78/78 [==============================] - 76s 969ms/step - loss: 1.5246 - accuracy: 0.5423 - val_loss: 1.4178 - val_accuracy: 0.5731\n",
      "Epoch 39/50\n",
      "78/78 [==============================] - 75s 957ms/step - loss: 1.5709 - accuracy: 0.5310 - val_loss: 1.4038 - val_accuracy: 0.5707\n",
      "Epoch 40/50\n",
      "78/78 [==============================] - 75s 967ms/step - loss: 1.5286 - accuracy: 0.5444 - val_loss: 1.3894 - val_accuracy: 0.5715\n",
      "Epoch 41/50\n",
      "78/78 [==============================] - 75s 958ms/step - loss: 1.4872 - accuracy: 0.5578 - val_loss: 1.3456 - val_accuracy: 0.5743\n",
      "Epoch 42/50\n",
      "78/78 [==============================] - 75s 966ms/step - loss: 1.5088 - accuracy: 0.5521 - val_loss: 1.4273 - val_accuracy: 0.5649\n",
      "Epoch 43/50\n",
      "78/78 [==============================] - 75s 962ms/step - loss: 1.5122 - accuracy: 0.5569 - val_loss: 1.3832 - val_accuracy: 0.5813\n",
      "Epoch 44/50\n",
      "78/78 [==============================] - 75s 965ms/step - loss: 1.4595 - accuracy: 0.5607 - val_loss: 1.3548 - val_accuracy: 0.5851\n",
      "Epoch 45/50\n",
      "78/78 [==============================] - 75s 962ms/step - loss: 1.6168 - accuracy: 0.5428 - val_loss: 1.6495 - val_accuracy: 0.5705\n",
      "Epoch 46/50\n",
      "78/78 [==============================] - 75s 961ms/step - loss: 1.5424 - accuracy: 0.5474 - val_loss: 1.5107 - val_accuracy: 0.5575\n",
      "Epoch 47/50\n",
      "78/78 [==============================] - 75s 960ms/step - loss: 1.5003 - accuracy: 0.5599 - val_loss: 1.3821 - val_accuracy: 0.5835\n",
      "Epoch 48/50\n",
      "78/78 [==============================] - 75s 961ms/step - loss: 1.5391 - accuracy: 0.5624 - val_loss: 1.3621 - val_accuracy: 0.5873\n",
      "Epoch 49/50\n",
      "78/78 [==============================] - 75s 957ms/step - loss: 1.4818 - accuracy: 0.5628 - val_loss: 1.3761 - val_accuracy: 0.5897\n",
      "Epoch 50/50\n",
      "78/78 [==============================] - 76s 980ms/step - loss: 1.4611 - accuracy: 0.5794 - val_loss: 1.2942 - val_accuracy: 0.6046\n",
      "Time taken by above cell is 63.572904801368715.\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 64\n",
    "# epc = 50\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# # training\n",
    "# train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "# step_size_train = train_generator.n // train_generator.batch_size\n",
    "\n",
    "# # validation\n",
    "# val_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "# step_size_val = val_generator.n // val_generator.batch_size\n",
    "\n",
    "# # test\n",
    "# test_generator = test_datagen.flow(x_test, y_test, batch_size=batch_size)\n",
    "# step_size_test = test_generator.n // test_generator.batch_size\n",
    "\n",
    "# # usefull callbacks\n",
    "# log_dir=\"Logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = 100000000)\n",
    "# lr_reducer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=7, min_lr=10e-7, min_delta=0.01, verbose=1)\n",
    "# early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15, verbose=1)\n",
    "\n",
    "# model.fit_generator(train_generator,\n",
    "#                     steps_per_epoch = step_size_train,\n",
    "#                     epochs = epc,\n",
    "#                     validation_data = val_generator,\n",
    "#                     validation_steps = step_size_val,\n",
    "#                     callbacks=[tensorboard_callback, lr_reducer, early_stopper])\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"Time taken by above cell is {}.\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "CltkRLQ5XYBV",
    "outputId": "687c78cc-4dff-4bd2-d459-2ef305be6f7a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d462b589da62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "val_scores = model.evaluate_generator(val_generator, verbose=0)\n",
    "test_scores = model.evaluate_generator(test_generator, verbose=0)\n",
    "print('validation loss:', val_scores[0])\n",
    "print('validation accuracy:', val_scores[1])\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoz0ZkTfXq9U"
   },
   "outputs": [],
   "source": [
    "model.save('Model56_paper.h5')  # 0.7418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0WNML-cW7j8j",
    "outputId": "f05bc283-254c-4931-bd58-49e044848e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m20191206-230234\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B88gCWdDysFV",
    "outputId": "b49d2731-5826-486e-ade9-cc8a52aa57e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Deep-Learning\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_paper56",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
